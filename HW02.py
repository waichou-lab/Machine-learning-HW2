# gpu_homework.py
import os
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

print("=" * 60)
print("æ·±åº¦ç¥ç¶“ç¶²çµ¡ä½œæ¥­")
print("=" * 60)

# è¨­ç½® GPU é…ç½®
def setup_gpu():
    # æª¢æŸ¥å¯ç”¨ GPU
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        print(f"ğŸ¯ æª¢æ¸¬åˆ° {len(gpus)} å€‹ GPU:")
        for gpu in gpus:
            print(f"   - {gpu}")
        
        try:
            # è¨­ç½® GPU å…§å­˜å¢é•·
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # è¨­ç½®é‚è¼¯ GPU
            logical_gpus = tf.config.experimental.list_logical_devices('GPU')
            print(f"âœ… é‚è¼¯ GPU: {len(logical_gpus)}")
            
            return True
        except RuntimeError as e:
            print(f"âš ï¸ GPU è¨­ç½®è­¦å‘Š: {e}")
            return False
    else:
        print("âŒ æœªæª¢æ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU")
        return False

# è¨­ç½® GPU
gpu_available = setup_gpu()

# åŠ è¼‰ CIFAR10 æ•¸æ“šé›†
print("\nğŸ“¦ åŠ è¼‰ CIFAR10 æ•¸æ“šé›†...")
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()

# æ•¸æ“šé è™•ç†
print("ğŸ”„ æ•¸æ“šé è™•ç†...")
X_train_full = X_train_full.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# å±•å¹³åœ–åƒ (32x32x3 = 3072)
X_train_full = X_train_full.reshape(-1, 32*32*3)
X_test = X_test.reshape(-1, 32*32*3)

# åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, test_size=0.1, random_state=42
)

print(f"ğŸ“Š æ•¸æ“šé›†ä¿¡æ¯:")
print(f"   è¨“ç·´é›†: {X_train.shape[0]} å€‹æ¨£æœ¬")
print(f"   é©—è­‰é›†: {X_valid.shape[0]} å€‹æ¨£æœ¬")
print(f"   æ¸¬è©¦é›†: {X_test.shape[0]} å€‹æ¨£æœ¬")
print(f"   ç‰¹å¾µç¶­åº¦: {X_train.shape[1]}")

# 1. åŸºç·šæ¨¡å‹ (20å±¤ DNNï¼ŒHeåˆå§‹åŒ–ï¼ŒELUæ¿€æ´»)
def build_baseline_model():
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(input_shape=(3072,)))
    
    # 20å€‹éš±è—å±¤ï¼Œæ¯å±¤100å€‹ç¥ç¶“å…ƒ
    for i in range(20):
        model.add(keras.layers.Dense(100, kernel_initializer="he_normal"))
        model.add(keras.layers.ELU())
    
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model

# 2. Batch Normalization æ¨¡å‹
def build_batchnorm_model():
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(input_shape=(3072,)))
    
    for i in range(20):
        model.add(keras.layers.Dense(100, kernel_initializer="he_normal"))
        model.add(keras.layers.BatchNormalization())
        model.add(keras.layers.ELU())
    
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model

# 3. SELU è‡ªæ­¸ä¸€åŒ–ç¶²çµ¡
def build_selu_model():
    # æ¨™æº–åŒ–è¼¸å…¥ç‰¹å¾µ (SELU çš„è¦æ±‚)
    scaler = StandardScaler()
    X_train_selu = scaler.fit_transform(X_train)
    X_valid_selu = scaler.transform(X_valid)
    X_test_selu = scaler.transform(X_test)
    
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(input_shape=(3072,)))
    
    for i in range(20):
        model.add(keras.layers.Dense(100, kernel_initializer="lecun_normal"))
        model.add(keras.layers.Activation("selu"))
    
    model.add(keras.layers.Dense(10, activation='softmax'))
    
    return model, X_train_selu, X_valid_selu, X_test_selu, scaler

# 4. Alpha Dropout æ¨¡å‹
def build_alpha_dropout_model():
    scaler = StandardScaler()
    X_train_alpha = scaler.fit_transform(X_train)
    X_valid_alpha = scaler.transform(X_valid)
    X_test_alpha = scaler.transform(X_test)
    
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(input_shape=(3072,)))
    
    for i in range(20):
        model.add(keras.layers.Dense(100, kernel_initializer="lecun_normal"))
        model.add(keras.layers.Activation("selu"))
        if i < 19:  # ä¸åœ¨æœ€å¾Œä¸€å±¤å‰æ·»åŠ  dropout
            model.add(keras.layers.AlphaDropout(rate=0.1))
    
    model.add(keras.layers.Dense(10, activation='softmax'))
    
    return model, X_train_alpha, X_valid_alpha, X_test_alpha, scaler

# å­¸ç¿’ç‡æŸ¥æ‰¾å‡½æ•¸
def find_learning_rate(model, X, y, validation_data, start_lr=1e-6, end_lr=1e-1, epochs=5):
    print("   ğŸ” å°‹æ‰¾æœ€ä½³å­¸ç¿’ç‡...")
    
    # æ¸¬è©¦å¹¾å€‹å­¸ç¿’ç‡
    learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]
    best_lr = 1e-3
    best_val_acc = 0
    
    for lr in learning_rates:
        model_copy = keras.models.clone_model(model)
        model_copy.build((None, 3072))
        model_copy.compile(
            optimizer=keras.optimizers.Nadam(learning_rate=lr),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # å¿«é€Ÿè¨“ç·´
        history = model_copy.fit(
            X, y,
            validation_data=validation_data,
            epochs=2,
            batch_size=128,
            verbose=0
        )
        
        val_acc = max(history.history['val_accuracy'])
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_lr = lr
    
    print(f"   âœ… æœ€ä½³å­¸ç¿’ç‡: {best_lr}")
    return best_lr

# è¨“ç·´å‡½æ•¸
def train_model(model, X_train, y_train, X_valid, y_valid, model_name, lr=0.001, epochs=100):
    print(f"\nğŸš€ è¨“ç·´ {model_name}...")
    
    model.compile(
        optimizer=keras.optimizers.Nadam(learning_rate=lr),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # æ—©åœæ³•
    early_stopping = keras.callbacks.EarlyStopping(
        patience=10,
        restore_best_weights=True,
        verbose=1
    )
    
    # ä½¿ç”¨ GPU å‹å¥½çš„ batch size
    batch_size = 512 if gpu_available else 128
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_valid, y_valid),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping],
        verbose=1
    )
    
    return history

# MC Dropout é æ¸¬
def mc_dropout_predict(model, X, n_samples=50):
    print(f"   ğŸ”„ MC Dropout é æ¸¬ ({n_samples} æ¬¡æ¡æ¨£)...")
    y_probas = []
    for i in range(n_samples):
        y_proba = model.predict(X, verbose=0)
        y_probas.append(y_proba)
    
    return np.mean(y_probas, axis=0)

# è©•ä¼°å‡½æ•¸
def evaluate_model(model, X_test, y_test, model_name, use_mc_dropout=False):
    if use_mc_dropout:
        y_proba = mc_dropout_predict(model, X_test)
        y_pred = np.argmax(y_proba, axis=1)
    else:
        y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    
    accuracy = accuracy_score(y_test, y_pred)
    mc_text = " (MC Dropout)" if use_mc_dropout else ""
    print(f"   ğŸ“Š {model_name}{mc_text} æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.4f}")
    return accuracy

# ä¸»è¨“ç·´æµç¨‹
print("\n" + "="*50)
print("é–‹å§‹æ¨¡å‹è¨“ç·´")
print("="*50)

# 1. åŸºç·šæ¨¡å‹
print("\n1ï¸âƒ£ åŸºç·šæ¨¡å‹ (20å±¤ DNN + ELU + Heåˆå§‹åŒ–)")
baseline_model = build_baseline_model()
baseline_lr = find_learning_rate(baseline_model, X_train, y_train, (X_valid, y_valid))
baseline_history = train_model(baseline_model, X_train, y_train, X_valid, y_valid, 
                              "åŸºç·šæ¨¡å‹", baseline_lr, 100)

# 2. BatchNorm æ¨¡å‹
print("\n2ï¸âƒ£ Batch Normalization æ¨¡å‹")
batchnorm_model = build_batchnorm_model()
batchnorm_lr = find_learning_rate(batchnorm_model, X_train, y_train, (X_valid, y_valid))
batchnorm_history = train_model(batchnorm_model, X_train, y_train, X_valid, y_valid,
                               "BatchNormæ¨¡å‹", batchnorm_lr, 100)

# 3. SELU æ¨¡å‹
print("\n3ï¸âƒ£ SELU è‡ªæ­¸ä¸€åŒ–æ¨¡å‹")
selu_model, X_train_selu, X_valid_selu, X_test_selu, selu_scaler = build_selu_model()
selu_lr = find_learning_rate(selu_model, X_train_selu, y_train, (X_valid_selu, y_valid))
selu_history = train_model(selu_model, X_train_selu, y_train, X_valid_selu, y_valid,
                          "SELUæ¨¡å‹", selu_lr, 100)

# 4. Alpha Dropout æ¨¡å‹
print("\n4ï¸âƒ£ Alpha Dropout æ¨¡å‹")
alpha_model, X_train_alpha, X_valid_alpha, X_test_alpha, alpha_scaler = build_alpha_dropout_model()
alpha_lr = find_learning_rate(alpha_model, X_train_alpha, y_train, (X_valid_alpha, y_valid))
alpha_history = train_model(alpha_model, X_train_alpha, y_train, X_valid_alpha, y_valid,
                           "AlphaDropoutæ¨¡å‹", alpha_lr, 100)

# è©•ä¼°æ‰€æœ‰æ¨¡å‹
print("\n" + "="*50)
print("æ¨¡å‹è©•ä¼°")
print("="*50)

baseline_acc = evaluate_model(baseline_model, X_test, y_test, "åŸºç·šæ¨¡å‹")
batchnorm_acc = evaluate_model(batchnorm_model, X_test, y_test, "BatchNormæ¨¡å‹")
selu_acc = evaluate_model(selu_model, X_test_selu, y_test, "SELUæ¨¡å‹")
alpha_acc = evaluate_model(alpha_model, X_test_alpha, y_test, "AlphaDropoutæ¨¡å‹")
alpha_mc_acc = evaluate_model(alpha_model, X_test_alpha, y_test, "AlphaDropoutæ¨¡å‹", use_mc_dropout=True)

# çµæœåˆ†æ
print("\n" + "="*50)
print("çµæœç¸½çµ")
print("="*50)

results = [
    ("åŸºç·šæ¨¡å‹", baseline_acc),
    ("BatchNormæ¨¡å‹", batchnorm_acc),
    ("SELUæ¨¡å‹", selu_acc),
    ("AlphaDropoutæ¨¡å‹", alpha_acc),
    ("AlphaDropout+MC", alpha_mc_acc)
]

print("ğŸ“ˆ æ¸¬è©¦æº–ç¢ºç‡:")
for name, acc in results:
    improvement = acc - baseline_acc
    print(f"   {name:20} {acc:.4f} ({improvement:+.4f})")

# ç¹ªè£½å­¸ç¿’æ›²ç·š
print("\nğŸ“Š ç¹ªè£½å­¸ç¿’æ›²ç·š...")
plt.figure(figsize=(16, 12))

# è¨“ç·´æå¤±
plt.subplot(2, 2, 1)
plt.plot(baseline_history.history['loss'], label='åŸºç·šæ¨¡å‹', linewidth=2)
plt.plot(batchnorm_history.history['loss'], label='BatchNormæ¨¡å‹', linewidth=2)
plt.plot(selu_history.history['loss'], label='SELUæ¨¡å‹', linewidth=2)
plt.plot(alpha_history.history['loss'], label='AlphaDropoutæ¨¡å‹', linewidth=2)
plt.title('è¨“ç·´æå¤±', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# é©—è­‰æº–ç¢ºç‡
plt.subplot(2, 2, 2)
plt.plot(baseline_history.history['val_accuracy'], label='åŸºç·šæ¨¡å‹', linewidth=2)
plt.plot(batchnorm_history.history['val_accuracy'], label='BatchNormæ¨¡å‹', linewidth=2)
plt.plot(selu_history.history['val_accuracy'], label='SELUæ¨¡å‹', linewidth=2)
plt.plot(alpha_history.history['val_accuracy'], label='AlphaDropoutæ¨¡å‹', linewidth=2)
plt.title('é©—è­‰æº–ç¢ºç‡', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# æ¸¬è©¦æº–ç¢ºç‡æ¯”è¼ƒ
plt.subplot(2, 2, 3)
model_names = [r[0] for r in results]
accuracies = [r[1] for r in results]
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

bars = plt.bar(model_names, accuracies, color=colors, alpha=0.7)
plt.title('æ¸¬è©¦æº–ç¢ºç‡æ¯”è¼ƒ', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y', alpha=0.3)

# æ·»åŠ æ•¸å€¼æ¨™ç±¤
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')

# æ”¶æ–‚é€Ÿåº¦åˆ†æ
plt.subplot(2, 2, 4)
convergence_data = [
    (len(baseline_history.history['loss']), 'åŸºç·šæ¨¡å‹'),
    (len(batchnorm_history.history['loss']), 'BatchNormæ¨¡å‹'),
    (len(selu_history.history['loss']), 'SELUæ¨¡å‹'),
    (len(alpha_history.history['loss']), 'AlphaDropoutæ¨¡å‹')
]

epochs = [d[0] for d in convergence_data]
names = [d[1] for d in convergence_data]

plt.bar(names, epochs, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.7)
plt.title('æ”¶æ–‚é€Ÿåº¦ (è¨“ç·´è¼ªæ•¸)', fontsize=14, fontweight='bold')
plt.ylabel('Epochs')
plt.xticks(rotation=45, ha='right')
plt.grid(True, axis='y', alpha=0.3)

for i, (epoch, name) in enumerate(zip(epochs, names)):
    plt.text(i, epoch + 0.5, str(epoch), ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
plt.show()

# æŠ€è¡“åˆ†æ
print("\n" + "="*50)
print("æŠ€è¡“åˆ†æ")
print("="*50)

print("1. ğŸ“Š Batch Normalization æ•ˆæœ:")
print(f"   - æ”¹é€²: {batchnorm_acc - baseline_acc:+.4f}")
print("   - è§€å¯Ÿ: æ‡‰è©²æœ‰æ›´å¿«çš„æ”¶æ–‚å’Œæ›´å¥½çš„æ³›åŒ–")

print("\n2. ğŸ”„ SELU è‡ªæ­¸ä¸€åŒ–æ•ˆæœ:")
print(f"   - æ”¹é€²: {selu_acc - baseline_acc:+.4f}")
print("   - è§€å¯Ÿ: ç„¡éœ€é¡¯å¼æ­¸ä¸€åŒ–å±¤çš„è‡ªæ­¸ä¸€åŒ–ç‰¹æ€§")

print("\n3. ğŸ›¡ï¸ Alpha Dropout æ•ˆæœ:")
print(f"   - åŸºç¤æ”¹é€²: {alpha_acc - baseline_acc:+.4f}")
print(f"   - MC Dropout é¡å¤–æ”¹é€²: {alpha_mc_acc - alpha_acc:+.4f}")
print("   - è§€å¯Ÿ: ç‚º SELU ç¶²çµ¡è¨­è¨ˆçš„æ­£å‰‡åŒ–")

print("\n4. âš¡ è¨“ç·´é€Ÿåº¦:")
print(f"   - åŸºç·šæ¨¡å‹: {len(baseline_history.history['loss'])} è¼ª")
print(f"   - BatchNorm: {len(batchnorm_history.history['loss'])} è¼ª")
print(f"   - SELU: {len(selu_history.history['loss'])} è¼ª")
print(f"   - AlphaDropout: {len(alpha_history.history['loss'])} è¼ª")

print(f"\n5. ğŸ–¥ï¸ ç¡¬ä»¶ä½¿ç”¨:")
print(f"   - GPU åŠ é€Ÿ: {'æ˜¯' if gpu_available else 'å¦'}")
if gpu_available:
    print(f"   - Batch Size: 512")
else:
    print(f"   - Batch Size: 128")

print("\nğŸ‰ ä½œæ¥­å®Œæˆï¼")
