# 深度神經網絡作業實驗報告

## 📊 實驗概覽

**實驗目標**：比較不同深度學習技術在CIFAR-10圖像分類任務上的效果  
**實驗環境**：CPU訓練，TensorFlow 2.20.0  
**數據集**：CIFAR-10（60,000張32×32彩色圖像）  
**網絡架構**：20層全連接網絡，每層100個神經元  

## 📈 實驗結果總表

| 模型 | 測試準確率 | 改進幅度 | 收斂輪數 | 性能評價 |
|------|------------|----------|----------|----------|
| 基線模型 | 0.5042 | - | 33 | 基準性能 |
| BatchNorm模型 | 0.4621 | -0.0421 | 19 | ❌ 性能下降 |
| SELU模型 | 0.4998 | -0.0044 | 18 | ⚡ 收斂最快 |
| AlphaDropout模型 | 0.2375 | -0.2667 | 11 | ❌ 嚴重過擬合 |
| AlphaDropout+MC | 0.2375 | -0.2667 | - | ❌ 無改善 |

## 🔬 詳細技術分析

### 1. Batch Normalization 效果分析

**預期效果**：
- 提升收斂速度
- 改善泛化能力  
- 穩定訓練過程

**實際結果**：
- 準確率：0.4621（下降4.21%）
- 收斂輪數：19輪（比基線快42%）
- 學習率：0.0001

**問題分析**：
- 可能引入過多噪聲
- 在深度網絡中可能導致梯度不穩定
- CPU環境可能影響BatchNorm的統計計算

### 2. SELU 自歸一化網絡

**配置**：
- 激活函數：SELU
- 初始化：LeCun正態初始化  
- 輸入標準化：是
- 學習率：0.001

**表現**：
- 準確率：0.4998（接近基線）
- 收斂速度：18輪（最快）
- 訓練穩定性：最佳

**優勢**：
- 無需顯式歸一化層
- 內置自歸一化特性
- 訓練過程最穩定

### 3. Alpha Dropout 分析

**配置**：
- Dropout率：0.1
- 激活函數：SELU
- MC Dropout採樣：50次

**問題表現**：
- 嚴重性能下降（-26.67%）
- 訓練困難，損失下降緩慢
- MC Dropout無額外改善

**可能原因**：
- Dropout率過高
- 對於深度網絡正則化過強
- 導致模型欠擬合

## 📊 性能可視化

### 準確率對比
基線模型: ██████████ 0.5042  
BatchNorm: ████████◐ 0.4621  
SELU: ██████████ 0.4998  
AlphaDropout: ██◐ 0.2375  

text

### 收斂速度對比
SELU: ██████████████████ (18輪)  
BatchNorm: █████████████████ (19輪)  
基線模型: █████████████████████████████ (33輪)  
AlphaDropout: ███████████ (11輪)  

text

## 🎯 關鍵發現

### ✅ 成功案例
1. **SELU自歸一化**：在保持性能的同時實現最快收斂
2. **早停法**：有效防止所有模型的過擬合  
3. **學習率搜索**：為不同模型找到合適的學習率

### ❌ 失敗案例
1. **BatchNorm**：在此特定設置下性能下降
2. **AlphaDropout**：正則化過強導致嚴重性能損失
3. **MC Dropout**：在AlphaDropout上無額外收益

## ⚙️ 超參數總結

| 參數 | 基線 | BatchNorm | SELU | AlphaDropout |
|------|------|-----------|------|--------------|
| 學習率 | 0.0001 | 0.0001 | 0.001 | 0.001 |
| Batch Size | 128 | 128 | 128 | 128 |
| 隱藏層 | 20×100 | 20×100 | 20×100 | 20×100 |
| 早停耐心 | 10 | 10 | 10 | 10 |
| 激活函數 | ELU | ELU | SELU | SELU |
| 初始化 | He | He | LeCun | LeCun |

## 🔮 改進建議

### 立即改進
1. **降低Dropout率**：嘗試0.05或0.02
2. **BatchNorm調參**：調整動量參數和epsilon值  
3. **學習率調度**：使用余弦退火或循環學習率

### 長期優化
1. **GPU環境**：在RTX 4050上重新實驗
2. **網絡架構**：嘗試ResNet或DenseNet
3. **數據增強**：添加圖像增強技術
4. **集成學習**：組合多個模型

## 📝 結論與建議

### 主要結論
1. **SELU是最佳選擇**：在深度網絡中表現穩定且收斂快
2. **正則化需要謹慎**：過強的正則化會嚴重損害性能  
3. **架構重要性**：20層全連接網絡可能不是CIFAR-10的最佳選擇

### 實踐建議
對於類似的圖像分類任務：
- 優先考慮SELU自歸一化網絡
- 謹慎使用BatchNorm，需要仔細調參
- Dropout率應從較小值開始嘗試  
- 始終使用早停法防止過擬合

---

**實驗完成時間**：2025年11月8日  
**總訓練時間**：約2小時  
**代碼行數**：400+  
**生成圖表**：4張綜合比較圖  
**備註**：所有實驗均使用相同的訓練/驗證/測試分割，確保結果可比性
