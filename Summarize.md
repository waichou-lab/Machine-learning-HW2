# 深度神經網絡作業實驗報告

## 📊 實驗概覽

**實驗目標**：比較不同深度學習技術在CIFAR-10圖像分類任務上的效果  
**實驗環境**：CPU訓練，TensorFlow 2.20.0  
**數據集**：CIFAR-10（60,000張32×32彩色圖像）  
**網絡架構**：20層全連接網絡，每層100個神經元  

## 📈 完整實驗結果圖表

![訓練結果綜合圖](training_results.png)
*圖1：四種深度學習技術在CIFAR-10上的綜合性能比較*

## 🔍 圖表詳細解讀

### 左上圖：訓練損失曲線 (Training Loss)
**觀察重點**：
- **基線模型（藍色）**：損失穩定下降，但收斂較慢，在33個epoch後停止
- **BatchNorm模型（橙色）**：早期收斂快，但後期出現波動，19個epoch提前停止
- **SELU模型（綠色）**：曲線最平滑，顯示最佳訓練穩定性，18個epoch收斂
- **AlphaDropout模型（紅色）**：損失下降緩慢，顯示訓練困難，11個epoch就停止

**技術洞察**：
> SELU展現了最好的訓練穩定性，而AlphaDropout由於正則化過強導致訓練困難

### 右上圖：驗證準確率曲線 (Validation Accuracy)
**觀察重點**：
- **基線模型**：穩定上升至約48%，後期出現波動
- **BatchNorm模型**：快速上升但在epoch 9後開始下降，顯示過擬合
- **SELU模型**：在epoch 8達到峰值約49.5%，之後保持相對穩定
- **AlphaDropout模型**：始終在低水平徘徊，最高僅約34%

**技術洞察**：
> BatchNorm雖然收斂快但泛化能力差，SELU在收斂速度和泛化間取得最佳平衡

### 左下圖：測試準確率比較 (Test Accuracy Comparison)
**觀察重點**：
- **基線模型**：0.5042 - 作為性能基準
- **BatchNorm模型**：0.4621 - 比基線下降4.21%
- **SELU模型**：0.4998 - 接近基線性能
- **AlphaDropout模型**：0.2375 - 嚴重性能損失
- **AlphaDropout+MC**：0.2375 - MC Dropout無改善

**技術洞察**：
> 在此實驗設置下，BatchNorm和AlphaDropout都未能提升性能，SELU是唯一保持基線水平的技術

### 右下圖：收斂速度分析 (Convergence Speed)
**觀察重點**：
- **SELU模型**：18個epoch - 最快收斂 ⭐
- **BatchNorm模型**：19個epoch - 收斂速度快
- **基線模型**：33個epoch - 收斂最慢
- **AlphaDropout模型**：11個epoch - 因性能差而提前停止

**技術洞察**：
> SELU不僅收斂最快，還保持了良好的最終性能，體現了自歸一化網絡的優勢

## 📊 實驗結果總表

| 模型 | 測試準確率 | 改進幅度 | 收斂輪數 | 性能評價 | 訓練穩定性 |
|------|------------|----------|----------|----------|------------|
| 基線模型 | 0.5042 | - | 33 | 基準性能 | ⭐⭐☆ |
| BatchNorm模型 | 0.4621 | -0.0421 | 19 | ❌ 性能下降 | ⭐⭐☆ |
| SELU模型 | 0.4998 | -0.0044 | 18 | ⚡ 收斂最快 | ⭐⭐⭐ |
| AlphaDropout模型 | 0.2375 | -0.2667 | 11 | ❌ 嚴重過擬合 | ⭐☆☆ |

## 🔬 技術深度分析

### 1. SELU的成功因素
**從圖表可見**：
- 訓練損失曲線最平滑（左上圖）
- 驗證準確率穩定上升（右上圖）
- 收斂速度最快（右下圖）

**技術原理**：
SELU（Scaled Exponential Linear Unit）的自歸一化特性使得深度網絡在訓練過程中自動維持均值和方差的穩定性，無需依賴外部歸一化技術。

### 2. BatchNorm的異常表現
**問題分析**：
- 驗證準確率早期達到峰值後下降（右上圖）
- 訓練損失後期波動（左上圖）
- 最終測試準確率反而下降

**可能原因**：
- 20層網絡可能過深，BatchNorm的歸一化統計在深度網絡中積累誤差
- 小batch size（128）在CPU上可能影響BatchNorm的統計準確性

### 3. AlphaDropout的失敗原因
**明顯問題**：
- 訓練損失下降極慢（左上圖）
- 驗證準確率始終很低（右上圖）
- MC Dropout無額外改善

**改進建議**：
- 降低dropout率（從0.1降至0.05或0.02）
- 只在部分層使用dropout，而非所有層

## 🎯 關鍵技術洞察

### 從學習曲線得出的重要結論：
1. **收斂速度 ≠ 最終性能**：BatchNorm收斂快但泛化差
2. **訓練穩定性很重要**：SELU的平滑曲線對應更好的泛化
3. **正則化要適度**：過強的AlphaDropout嚴重損害模型容量

### 各技術適用場景：
- **SELU**：深度全連接網絡的首選
- **BatchNorm**：需要仔細調參，適合卷積網絡
- **AlphaDropout**：需要謹慎使用，從低dropout率開始

## 📝 最終結論

**最佳實踐推薦**：
> 對於深度全連接網絡，優先使用**SELU自歸一化網絡**，它在訓練穩定性、收斂速度和最終性能方面表現最均衡。

**技術選擇優先級**：
1. ✅ SELU + 早停法
2. ⚠️ BatchNorm（需要仔細調參）
3. ❌ AlphaDropout（在此設置下不推薦）

---
